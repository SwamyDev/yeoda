========
Examples
========

The following examples shall help to understand what the package is able to accomplish
and how you can use the interface *yeoda* offers to access and play around with your data.


Contents
========

.. toctree::
   :maxdepth: 2

   Setting up a data cube
   Module Reference
   License
   Authors
   Changelog


Setting up a data cube
======================
In simple words, *yeoda* is a filename based data cube tool, which means that it tries to interpret the data structure via the filename.
In the future it will be also possible to create a data cube based on metadata or dataset attributes. To define a filenaming convention,
*geopathfinder* can be used. Each (existing) filenaming convention has a ``create_[naming_convention]_filename(...)`` function to create a Python object,
which can be handled like a dictionary to access parts of the filename.

First, to setup a data cube, you need to prepare some input attributes:
  * A list of filepaths with the same extension (``filepaths``). Currently GeoTIFF and NetCDF files are supported as default by *veranda*.
  * A list of dimensions you want you work with. The dimension names relate to the keys defined by filenaming convention, e.g.:
    ``dimensions = ['time', 'var_name', 'pol']``
  * A function to create a Python object/class instance representing a filenaming convention, e.g.:
    ``smart_filename_creator = create_sgrt_filename``
  * A grid/tiled projection system, which is a class instance of ``pytileproj.base.TiledProjection`` being inherited to a grid package, e.g. *Equi7Grid*:
    ``grid = Equi7Grid(10).EU``

You can then initiate a data cube object with the ``EODataCube`` class:

.. code-block:: python
   dc = EODataCube(filepaths=filepaths, smart_filename_creator=smart_filename_creator,
                   dimensions=dimensions, grid=grid)

We have also prepared some higher-level data cubes, especially designed to work with (backscatter) products generated by the research group Remote Sensing of the GEO Department at TU Wien (TUWGEO).
To work with preprocessed data you can use the classes ``SIG0DataCube`` for sigma nought and ``GMRDataCube`` for radiometric terrain flattened gamma nought data.
On the value-added data side, ``SSMDataCube`` allows you to access the TUWGEO SSM data.

Data cube operations
====================
Now we can use our initialised data cubes to work with our data.
*yeoda* uses a *GeoPandas* dataframe to store the filename and geometry information internally.
On top of that, data cube functions where defined to filter, split, sort, align, etc. the data.
It has to be noted that most functions have a keyword argument ``in_place``.
If it is set to true, the original object will be overwritten.
If not, a new data cube object will be returned.
In the next sections some of these functions will be shortly described.

Renaming a dimension
--------------------
If you have to work with a pre-defined naming convention in *geopathfinder* (e.g. the SGRT naming convention)
and if you do not agree with the naming of the filename parts/dimensions, you can still rename dimensions afterwards:

.. code-block:: python
   dimensions_map = {'tile_name': 'tile'}
   dc.rename_dimension(dimensions_map, in_place=True)

In the example above, the dimension "tile_name" was renamed to "tile".

Adding a dimension
------------------
You can simply add new filepath-dependent values (e.g. file size, cloud coverage, ...) along a new dimension (e.g. named "new_dimension")
with a few lines of code:
.. code-block:: python
   values = ... # list containing values equal to len(dc)
   dc.add_dimension("new_dimension", values, in_place=True)

Sorting along a dimension
-------------------------
Sorting along a dimension can be accomplished with:

.. code-block:: python
   dc.sort_by_dimension('time', ascending=True, in_place=True)

All rows with respect to the values along "time" are now sorted in ascending order.

Filter by metadata
------------------
If you have stored metadata attributes in you NetCDF or GeoTIFF files, you can also filter the data cube by certain attributes.

.. code-block:: python
   metadata = {'creator': 'me'}
   dc.filter_by_metadata(metadata, in_place=True)

After executing the code above, ``dc`` only stores file where a metadata attribute "creator" is equal to "me".

Filter by dimension
-------------------
A very important function is ``filter_by_dimension``, which accepts a list of values and expressions to filter the data along a dimension.
``expressions`` is a list having the same length as ``values`` and includes mathematical comparison operators, i.e. "==", "<=", ">=", "<", ">" ("==" is default).
Some examples are:

.. code-block:: python
   values = ['VV']
   dc.filter_by_dimension(self, values, name="pol", in_place=True)

The command above would limit the data cube to only store entries where the dimension 'pol' contains 'VV' values.
Next, we could also filter by time ranges, i.e. to only allow values between 01-01-2016 and 01-02-2016:

.. code-block:: python
   from datetime import datetime
   start_time = datetime.strptime('01-01-2016', '%Y-%m-%d')
   end_time = datetime.strptime('01-02-2016', '%Y-%m-%d')
   values = [(start_time, end_time)]
   expressions = [('>=', '<=')]
   dc.filter_by_dimension(self, values, expressions=expressions, name="pol", in_place=True)

Split by dimension
------------------
``split_by_dimension`` works very similar to ``filter_by_dimension``, but now all filtered values are also split up into new data cubes:
    .. code-block:: python
       values = ['VV', 'VH']
       dc_vv, dc_vh = dc.split_by_dimension(self, values, name="pol")
where ``dc_vv`` is now a data cube only containing VV data and ``dc_vh`` a data cube only containing VH.


Loading data
============
Loading data by coordinates
---------------------------
Loading data by pixels
----------------------
Loading data by geometries
--------------------------
